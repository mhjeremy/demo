{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451525f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (1.26.1)\n",
      "Requirement already satisfied: requests in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (2.29.0)\n",
      "Requirement already satisfied: torch in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (2.0.1)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl (907 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m907.0/907.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: pandas in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: filelock in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/hejunjeremymao/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy requests torch tiktoken matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69831c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e327eff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17a8a1130>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 4  # How many batches per training step\n",
    "context_length = 16  # Length of the token chunk each batch\n",
    "d_model = 64  # The vector size of the token embeddings\n",
    "num_layers = 8  # Number of transformer blocks\n",
    "num_heads = 4  # Number of heads in Multi-head attention # d_model / num_heads = head_size\n",
    "learning_rate = 1e-3  # 0.001\n",
    "dropout = 0.1 # Dropout rate\n",
    "max_iters = 5000  # Total of training iterations\n",
    "eval_interval = 50  # How often to evaluate the model\n",
    "eval_iters = 20  # How many iterations to average the loss over when evaluating the model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Instead of using the cpu, we'll use the GPU if it's available.\n",
    "\n",
    "TORCH_SEED = 1337\n",
    "torch.manual_seed(TORCH_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9594eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a sample txt file from https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt\n",
    "if not os.path.exists('sales_textbook.txt'):\n",
    "    url = 'https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt'\n",
    "    with open('sales_textbook.txt', 'w') as f:\n",
    "        f.write(requests.get(url).text)\n",
    "\n",
    "with open('sales_textbook.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a508b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text size: 77919\n",
      "Vocabulary size: 77919\n",
      "The maximum value in the tokenized text is: 100069\n"
     ]
    }
   ],
   "source": [
    "# Using TikToken to tokenize the source text\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenized_text = encoding.encode(text) # size of tokenized source text is 77,919\n",
    "tokenized_text = torch.tensor(tokenized_text, dtype = torch.long)\n",
    "vocab_size = len(set(tokenized_text)) # size of vocabulary is 3,771\n",
    "max_token_value = max(tokenized_text)\n",
    "\n",
    "print(f\"Tokenized text size: {len(tokenized_text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"The maximum value in the tokenized text is: {max_token_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ec7bbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16]) torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "# Split train and validation\n",
    "split_idx = int(len(tokenized_text) * 0.8)\n",
    "train_data = tokenized_text[:split_idx]\n",
    "val_data = tokenized_text[split_idx:]\n",
    "\n",
    "# Prepare data for training batch\n",
    "# Prepare data for training batch\n",
    "data = train_data\n",
    "idxs = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))\n",
    "x_batch = torch.stack([data[idx:idx + context_length] for idx in idxs])\n",
    "y_batch = torch.stack([data[idx + 1:idx + context_length + 1] for idx in idxs])\n",
    "print(x_batch.shape, x_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66843ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Token Embedding look-up table\n",
    "token_embedding_lookup_table = nn.Embedding(max_token_value, d_model)\n",
    "\n",
    "# Get X and Y embedding\n",
    "x = token_embedding_lookup_table(x_batch.data)\n",
    "y = token_embedding_lookup_table(y_batch.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7180bea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Encoding Look-up Table:  torch.Size([4, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "# Define Position Encoding look-up table\n",
    "position_encoding_lookup_table = torch.zeros(context_length, d_model) # initial with zeros with shape (context_length, d_model)\n",
    "position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "# apply the sine & cosine\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)\n",
    "position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)\n",
    "position_encoding_lookup_table = position_encoding_lookup_table.unsqueeze(0).expand(batch_size, -1, -1) #add batch to the first dimension\n",
    "\n",
    "print(\"Position Encoding Look-up Table: \", position_encoding_lookup_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94636c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Input Embedding of x: \n",
      "           0         1         2         3         4         5         6   \\\n",
      "0  -0.299130  0.949924 -0.631772  0.939232 -2.082235  1.398602 -0.240277   \n",
      "1   0.758386  0.999339  0.238804  2.130460 -0.736705  1.686566  0.824980   \n",
      "2  -0.922071 -2.250749 -0.556319  1.222940  2.109022 -0.400113 -0.138789   \n",
      "3   0.306074 -1.573041  2.231532  0.032769  1.850678 -0.120396 -0.011787   \n",
      "4  -0.954794 -0.976841  0.139319 -0.865764 -0.193849 -1.655313  1.521742   \n",
      "5   0.679198  0.506228  0.353623 -0.863025 -2.570290 -1.323900  0.371723   \n",
      "6  -0.323705  2.691280 -2.467007  0.717207  1.387555 -2.344249  0.049187   \n",
      "7   0.476250  1.532691 -0.440719  0.884680 -0.049779 -0.415280 -0.095141   \n",
      "8  -1.377338 -0.241152  0.619716  0.535313 -2.747910 -0.546994 -1.181236   \n",
      "9  -0.405825  0.559437  1.838797  0.457612 -2.470442  1.303259 -0.322104   \n",
      "10 -0.724732 -1.171013 -0.256460 -1.604869 -1.592075  0.130025  0.570488   \n",
      "11 -0.449413 -0.703517  0.179154  0.373091 -0.182106  1.931234 -0.799360   \n",
      "12  0.341896  1.383503 -0.627911 -1.276247  1.170495  1.140587 -1.435867   \n",
      "13  0.524299  0.225087 -1.034895  0.773311  2.538593  0.801516 -2.413383   \n",
      "14  1.239190 -0.703228 -0.857715 -0.738189  2.185374  1.034195  0.174827   \n",
      "15  2.108387 -0.318655 -0.287144  0.649548  0.771553 -0.321154  1.457626   \n",
      "\n",
      "          7         8         9   ...        54        55        56        57  \\\n",
      "0   1.460907  0.438821  0.568639  ...  1.452847  1.467532  1.984371  0.791459   \n",
      "1   0.610698 -0.504885  1.616139  ...  0.448665 -2.071404  0.672218  2.796822   \n",
      "2  -1.933866 -1.671411  1.150713  ... -0.686295 -0.096573 -2.178207  0.451573   \n",
      "3  -0.635150  0.029269  1.307228  ... -0.607196  2.235354 -0.330132  0.043395   \n",
      "4  -0.318029 -0.214993  0.665824  ... -0.545532 -0.345191  0.142483  1.507527   \n",
      "5  -1.104829  0.616293  0.783596  ...  1.080138 -0.161667 -1.531240  1.557224   \n",
      "6   0.084035  1.446481  0.074499  ...  0.716758  1.740306 -1.434396  0.254191   \n",
      "7  -1.715202 -0.558719 -1.187362  ...  0.746224  0.008394 -0.101539 -0.490031   \n",
      "8  -3.191279 -0.665482 -1.169012  ... -0.079880  3.122454 -2.058005  1.065758   \n",
      "9   0.723305 -1.070696 -1.178379  ... -0.280317  1.782857 -1.054796  1.835647   \n",
      "10 -1.642120  0.801727 -1.764521  ... -0.340848  3.639359 -0.644349  3.063182   \n",
      "11  0.825862 -0.736930  0.177177  ... -0.280089  1.006504  1.357095  1.059691   \n",
      "12  1.224846 -0.545266 -1.121737  ... -1.307939  2.227912 -1.178179  1.575556   \n",
      "13 -1.562514  0.118980  0.325632  ...  0.397413  0.479963  0.217271  2.055336   \n",
      "14  1.099753 -1.613777 -1.243054  ...  0.359625  1.258582  1.067245  1.663585   \n",
      "15  0.268369 -0.826254 -0.871298  ...  1.625291  0.643504 -1.032252  1.950458   \n",
      "\n",
      "          58        59        60        61        62        63  \n",
      "0   0.456751  0.789546 -2.045897  1.099113  0.166035  3.139895  \n",
      "1   0.437217  2.284778 -1.708361 -0.152342  0.699953  0.343699  \n",
      "2  -0.500309  1.891739  0.073773  0.102590 -0.138560  0.840906  \n",
      "3   1.379010  1.228062 -0.214477  0.090425 -0.173942 -0.830836  \n",
      "4   0.027854 -0.008453 -1.213767 -0.094705  0.886935  1.103025  \n",
      "5  -0.348979  1.005130 -0.339587  0.136550 -0.597760  1.012034  \n",
      "6  -0.240991  0.542596  0.039506  2.375268  0.122078  1.598531  \n",
      "7  -0.253113  1.570766 -0.642553  0.589649 -0.934843 -1.100760  \n",
      "8   0.787592  2.288829  0.152384  1.582828 -0.278190  1.500612  \n",
      "9  -2.389836  2.359888 -0.895393  2.478489  1.150637  2.515316  \n",
      "10  1.745668  1.311434 -0.703046  0.981743  0.178330  0.443149  \n",
      "11  1.175118  1.527424 -0.844068  1.202420 -0.927549  0.908416  \n",
      "12  0.974123  1.321630 -0.581522 -1.142979 -0.997341  2.858498  \n",
      "13  0.841140  0.250887  1.338835  1.003394  1.427358  1.072308  \n",
      "14  0.892081  1.761903  0.728198  1.667248 -1.340948  0.224300  \n",
      "15  0.948557  2.855112 -0.750874  2.738219  0.865055  2.720319  \n",
      "\n",
      "[16 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# Add positional encoding into the input embedding vector\n",
    "input_embedding_x = x + position_encoding_lookup_table # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "input_embedding_y = y + position_encoding_lookup_table\n",
    "\n",
    "X = input_embedding_x\n",
    "\n",
    "x_plot = input_embedding_x[0].detach().cpu().numpy()\n",
    "print(\"Final Input Embedding of x: \\n\", pd.DataFrame(x_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd677a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Query, Key, Value for Multi-head Attention\n",
    "\n",
    "query = key = value = X # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "\n",
    "# Define Query, Key, Value weight matrices\n",
    "Wq = nn.Linear(d_model, d_model)\n",
    "Wk = nn.Linear(d_model, d_model)\n",
    "Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Wq(query) #[4, 16, 64]\n",
    "Q = Q.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]\n",
    "\n",
    "K = Wk(key) #[4, 16, 64]\n",
    "K = K.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]\n",
    "\n",
    "V = Wv(value) #[4, 16, 64]\n",
    "V = V.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116ced0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose q,k,v from [batch_size, context_length, num_heads, head_size] to [batch_size, num_heads, context_length, head_size]\n",
    "# The reason is that treat each batch with \"num_heads\" as its first dimension.\n",
    "Q = Q.transpose(1, 2) # [4, 4, 16, 16]\n",
    "K = K.transpose(1, 2) # [4, 4, 16, 16]\n",
    "V = V.transpose(1, 2) # [4, 4, 16, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6de1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the attention score betwee Q and K^T\n",
    "attention_score = torch.matmul(Q, K.transpose(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a34ad794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then Scale the attention score by the square root of the head size\n",
    "attention_score = attention_score / math.sqrt(d_model // num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4033f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6   \\\n",
      "0   0.021498  0.759700  1.114896  0.861203  0.996177  0.087225  0.020397   \n",
      "1  -0.186354  0.423521  0.930679  0.397063  0.689583 -0.023029 -1.295207   \n",
      "2  -0.116987 -0.521399 -0.102028 -0.048671 -0.094188  0.211903  0.785083   \n",
      "3   0.677379  0.094782  0.187630  1.087047  0.387744  0.641712  1.120173   \n",
      "4   0.631352 -0.280073 -0.932875 -0.581271 -0.206257  0.127697 -0.102794   \n",
      "5  -0.327146  0.068143  0.148191 -0.391794  0.132223  0.017728  0.268415   \n",
      "6  -0.444339  0.258554  0.455449 -0.077464  0.134794 -0.053327  0.101351   \n",
      "7  -0.245799  0.580801  0.932249  0.135226  0.357541 -0.006203  0.136920   \n",
      "8  -0.303093  0.018890  0.015307  0.294602  0.369652 -0.219553 -0.192504   \n",
      "9  -0.136668  0.443592  0.470723 -0.184153  0.221327 -0.058326 -0.335395   \n",
      "10  0.694354  0.946938  0.422883  0.789916  0.990797 -0.172340  0.704143   \n",
      "11  0.352159  0.977470  1.040248  1.317057  1.359986  0.120361  0.799884   \n",
      "12 -0.238808  0.104332  0.052890 -0.593304 -0.794684 -0.462218  0.311895   \n",
      "13 -0.517919  0.067903  0.636224  0.144879  0.018605  0.213238  0.280446   \n",
      "14 -0.085574 -0.059927  0.476232 -0.008881 -0.143431  0.318674  0.256556   \n",
      "15 -0.184219 -0.247370  0.608031  0.304992  0.352951  0.453829  0.285533   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0   0.353933 -0.263620 -0.481791 -1.015287 -0.027799 -0.770300  1.028328   \n",
      "1   0.035366 -0.226877 -1.094293 -0.077335  0.149487 -1.291373 -0.246792   \n",
      "2   0.184757 -0.163499  0.059236  0.515760  0.117891  0.133373  0.033434   \n",
      "3   0.259390 -0.339914  0.120134 -0.394649 -0.246437 -0.579486  0.994677   \n",
      "4  -0.442630  0.287699 -0.108314  0.540231  0.316796 -0.293156 -0.427342   \n",
      "5   0.222270  0.331641 -0.015495 -0.366638  0.014746 -0.224710  0.378336   \n",
      "6   0.162352  0.333664  0.069485 -0.037877 -0.015095 -0.301473  0.237264   \n",
      "7   0.361666  0.171941 -0.240028 -0.460423 -0.134655  0.122417  0.401689   \n",
      "8  -0.606256 -0.548382 -0.053605 -0.832274 -0.339201 -0.297512  0.289678   \n",
      "9   0.530909  0.210423  0.113577 -0.280486  0.202264  0.089872  0.381280   \n",
      "10 -0.240142 -0.371990 -0.299596 -0.832198 -0.359611 -0.637496  0.765209   \n",
      "11  0.680050  0.324914 -0.482536 -0.499427  0.151246 -0.254691  1.035710   \n",
      "12  0.081620 -0.338989 -0.082385 -0.711630 -0.425134  0.018855  0.009447   \n",
      "13  0.551979  0.049975 -0.170539  0.008342  0.121067  0.532295  0.402487   \n",
      "14  0.872132  0.463599  0.522785  0.704850  0.512636  0.430275  0.164187   \n",
      "15  0.616212  0.059369  0.419577  0.663161  0.387546  0.333471  0.249560   \n",
      "\n",
      "          14        15  \n",
      "0   0.241509  0.527406  \n",
      "1  -0.591006  0.052637  \n",
      "2   0.032123  0.521154  \n",
      "3  -0.021960  0.356922  \n",
      "4  -0.393042 -0.474840  \n",
      "5   0.081683  0.197241  \n",
      "6   0.111261  0.401928  \n",
      "7   0.278084  0.003031  \n",
      "8   0.017493  0.183737  \n",
      "9   0.191927  0.236189  \n",
      "10  0.152228  0.264614  \n",
      "11  0.668065  0.268745  \n",
      "12 -0.214531 -0.060118  \n",
      "13  0.094902  0.129446  \n",
      "14  0.546852  0.545444  \n",
      "15  0.546221  0.853163  \n"
     ]
    }
   ],
   "source": [
    "attention_score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model // num_heads) # [4, 4, 16, 16] #[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
    "print(pd.DataFrame(attention_score[0][0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "094c9f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6   \\\n",
      "0   0.021498      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "1  -0.186354  0.423521      -inf      -inf      -inf      -inf      -inf   \n",
      "2  -0.116987 -0.521399 -0.102028      -inf      -inf      -inf      -inf   \n",
      "3   0.677379  0.094782  0.187630  1.087047      -inf      -inf      -inf   \n",
      "4   0.631352 -0.280073 -0.932875 -0.581271 -0.206257      -inf      -inf   \n",
      "5  -0.327146  0.068143  0.148191 -0.391794  0.132223  0.017728      -inf   \n",
      "6  -0.444339  0.258554  0.455449 -0.077464  0.134794 -0.053327  0.101351   \n",
      "7  -0.245799  0.580801  0.932249  0.135226  0.357541 -0.006203  0.136920   \n",
      "8  -0.303093  0.018890  0.015307  0.294602  0.369652 -0.219553 -0.192504   \n",
      "9  -0.136668  0.443592  0.470723 -0.184153  0.221327 -0.058326 -0.335395   \n",
      "10  0.694354  0.946938  0.422883  0.789916  0.990797 -0.172340  0.704143   \n",
      "11  0.352159  0.977470  1.040248  1.317057  1.359986  0.120361  0.799884   \n",
      "12 -0.238808  0.104332  0.052890 -0.593304 -0.794684 -0.462218  0.311895   \n",
      "13 -0.517919  0.067903  0.636224  0.144879  0.018605  0.213238  0.280446   \n",
      "14 -0.085574 -0.059927  0.476232 -0.008881 -0.143431  0.318674  0.256556   \n",
      "15 -0.184219 -0.247370  0.608031  0.304992  0.352951  0.453829  0.285533   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "1       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "2       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "3       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "4       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "5       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "6       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "7   0.361666      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "8  -0.606256 -0.548382      -inf      -inf      -inf      -inf      -inf   \n",
      "9   0.530909  0.210423  0.113577      -inf      -inf      -inf      -inf   \n",
      "10 -0.240142 -0.371990 -0.299596 -0.832198      -inf      -inf      -inf   \n",
      "11  0.680050  0.324914 -0.482536 -0.499427  0.151246      -inf      -inf   \n",
      "12  0.081620 -0.338989 -0.082385 -0.711630 -0.425134  0.018855      -inf   \n",
      "13  0.551979  0.049975 -0.170539  0.008342  0.121067  0.532295  0.402487   \n",
      "14  0.872132  0.463599  0.522785  0.704850  0.512636  0.430275  0.164187   \n",
      "15  0.616212  0.059369  0.419577  0.663161  0.387546  0.333471  0.249560   \n",
      "\n",
      "          14        15  \n",
      "0       -inf      -inf  \n",
      "1       -inf      -inf  \n",
      "2       -inf      -inf  \n",
      "3       -inf      -inf  \n",
      "4       -inf      -inf  \n",
      "5       -inf      -inf  \n",
      "6       -inf      -inf  \n",
      "7       -inf      -inf  \n",
      "8       -inf      -inf  \n",
      "9       -inf      -inf  \n",
      "10      -inf      -inf  \n",
      "11      -inf      -inf  \n",
      "12      -inf      -inf  \n",
      "13      -inf      -inf  \n",
      "14  0.546852      -inf  \n",
      "15  0.546221  0.853163  \n"
     ]
    }
   ],
   "source": [
    "# Apply Mask to attention scores\n",
    "attention_score = attention_score.masked_fill(torch.triu(torch.ones(attention_score.shape[-2:]), diagonal=1).bool(), float('-inf')) #[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
    "print(pd.DataFrame(attention_score[0][0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cde5d15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6   \\\n",
      "0   0.153417  0.056439  0.056439  0.056439  0.056439  0.056439  0.056439   \n",
      "1   0.082039  0.110280  0.057691  0.057691  0.057691  0.057691  0.057691   \n",
      "2   0.084435  0.074588  0.084910  0.058159  0.058159  0.058159  0.058159   \n",
      "3   0.076467  0.067816  0.068825  0.087754  0.058261  0.058261  0.058261   \n",
      "4   0.089366  0.069222  0.063753  0.066199  0.070138  0.058302  0.058302   \n",
      "5   0.066270  0.070397  0.071490  0.065755  0.071264  0.069759  0.058506   \n",
      "6   0.063675  0.069366  0.071973  0.066089  0.068010  0.066285  0.067676   \n",
      "7   0.062778  0.068641  0.073386  0.064833  0.066495  0.063969  0.064844   \n",
      "8   0.064047  0.066245  0.066216  0.068873  0.069746  0.064544  0.064715   \n",
      "9   0.063119  0.066889  0.067132  0.062903  0.065155  0.063499  0.062285   \n",
      "10  0.066168  0.068513  0.064288  0.066977  0.068994  0.061678  0.066247   \n",
      "11  0.062265  0.065606  0.066086  0.068658  0.069135  0.061495  0.064417   \n",
      "12  0.063096  0.065002  0.064669  0.061742  0.061170  0.062185  0.066557   \n",
      "13  0.060748  0.062441  0.065476  0.062752  0.062255  0.063050  0.063365   \n",
      "14  0.061220  0.061287  0.063194  0.061426  0.061075  0.062518  0.062281   \n",
      "15  0.060799  0.060669  0.063429  0.062158  0.062333  0.062731  0.062090   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0   0.056439  0.056439  0.056439  0.056439  0.056439  0.056439  0.056439   \n",
      "1   0.057691  0.057691  0.057691  0.057691  0.057691  0.057691  0.057691   \n",
      "2   0.058159  0.058159  0.058159  0.058159  0.058159  0.058159  0.058159   \n",
      "3   0.058261  0.058261  0.058261  0.058261  0.058261  0.058261  0.058261   \n",
      "4   0.058302  0.058302  0.058302  0.058302  0.058302  0.058302  0.058302   \n",
      "5   0.058506  0.058506  0.058506  0.058506  0.058506  0.058506  0.058506   \n",
      "6   0.058547  0.058547  0.058547  0.058547  0.058547  0.058547  0.058547   \n",
      "7   0.066530  0.058566  0.058566  0.058566  0.058566  0.058566  0.058566   \n",
      "8   0.062576  0.062821  0.058603  0.058603  0.058603  0.058603  0.058603   \n",
      "9   0.067698  0.065081  0.064457  0.058630  0.058630  0.058630  0.058630   \n",
      "10  0.061472  0.061112  0.061304  0.060178  0.058614  0.058614  0.058614   \n",
      "11  0.063736  0.062164  0.060180  0.060154  0.061587  0.058629  0.058629   \n",
      "12  0.064853  0.062660  0.063878  0.061392  0.062321  0.064459  0.058672   \n",
      "13  0.064900  0.062372  0.061625  0.062218  0.062653  0.064772  0.063999   \n",
      "14  0.065502  0.063136  0.063417  0.064404  0.063368  0.062985  0.061956   \n",
      "15  0.063469  0.061389  0.062591  0.063707  0.062465  0.062261  0.061967   \n",
      "\n",
      "          14        15  \n",
      "0   0.056439  0.056439  \n",
      "1   0.057691  0.057691  \n",
      "2   0.058159  0.058159  \n",
      "3   0.058261  0.058261  \n",
      "4   0.058302  0.058302  \n",
      "5   0.058506  0.058506  \n",
      "6   0.058547  0.058547  \n",
      "7   0.058566  0.058566  \n",
      "8   0.058603  0.058603  \n",
      "9   0.058630  0.058630  \n",
      "10  0.058614  0.058614  \n",
      "11  0.058629  0.058629  \n",
      "12  0.058672  0.058672  \n",
      "13  0.058687  0.058687  \n",
      "14  0.063537  0.058696  \n",
      "15  0.063135  0.064807  \n"
     ]
    }
   ],
   "source": [
    "# Softmax the attention score\n",
    "attention_score = torch.softmax(attention_score, dim=-1)  #[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
    "print(pd.DataFrame(attention_score[0][0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dd66c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Calculate the V attention output\n",
    "A = torch.matmul(attention_score, V) # [4, 4, 16, 16] [batch_size, num_heads, context_length, head_size]\n",
    "print(attention_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "676c9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A.transpose(1, 2) # [4, 16, 4, 16] [batch_size, context_length, num_heads, head_size]\n",
    "A = A.reshape(batch_size, -1, d_model) # [4, 16, 64] [batch_size, context_length, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c95d75e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "# Define the output weight matrix\n",
    "Wo = nn.Linear(d_model, d_model)\n",
    "output = Wo(A) # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "462573fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add residual connection\n",
    "output = output + X\n",
    "\n",
    "# Add Layer Normalization\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "output = layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cab6453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feed Forward Network\n",
    "output = nn.Linear(d_model, d_model * 4)(output)\n",
    "output = nn.ReLU()(output)\n",
    "output = nn.Linear(d_model * 4, d_model)(output)\n",
    "output = torch.dropout(output, p=dropout, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "462051fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add residual connection\n",
    "output = output + X\n",
    "# Add Layer Normalization\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "output = layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15810484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0         1         2         3         4         5         6       \\\n",
      "0   0.573332  1.015899 -0.222024 -0.508197 -0.539159  0.016217 -0.901942   \n",
      "1   1.419667 -0.440910  0.260394 -0.632957  0.630532 -1.415623  0.429661   \n",
      "2   0.487616  0.048311  0.370451 -0.152401 -1.184079  0.517175 -0.095659   \n",
      "3   0.347335  0.663217 -0.251170 -0.163551 -0.734540  0.343969 -0.334619   \n",
      "4   0.242071  1.221849  0.048497  0.264043 -0.003944  0.361158 -1.098022   \n",
      "5  -1.488472 -0.487731 -1.028909 -0.285336 -0.071531  0.363977 -0.032143   \n",
      "6  -0.350975  0.324540 -0.418890  0.509512 -0.674422  0.128509 -0.785958   \n",
      "7   0.026156 -0.702553 -0.197620 -0.669560 -0.433044 -0.749262 -0.609470   \n",
      "8  -0.141884  1.253866 -0.156874 -1.094536 -0.360480  0.602933  0.021170   \n",
      "9  -0.297196  0.157524  0.435299 -0.918095 -0.514047 -0.618292  0.317165   \n",
      "10 -0.651161 -0.437216  0.192381 -1.242731 -0.336959  0.537993 -0.729907   \n",
      "11 -0.581308 -0.284026 -0.127426 -1.490744 -0.619038  0.568174 -0.286749   \n",
      "12  0.112364  0.109640  0.674520  0.200194 -1.289201  0.564952  0.462781   \n",
      "13  1.005109  0.434439  0.296782 -0.936538 -0.023256  0.394058 -0.898952   \n",
      "14  0.527231 -0.709480 -0.109826 -0.084028 -0.808104 -0.269907 -1.034420   \n",
      "15 -0.249850 -0.324798 -0.325676 -0.300474 -0.997311 -0.131739 -0.658156   \n",
      "\n",
      "      7         8         9       ...    100059    100060    100061    100062  \\\n",
      "0  -0.963477  0.026041 -0.137340  ...  0.257053 -0.517845  1.848028 -0.814542   \n",
      "1  -0.539343  0.620942  0.330925  ...  0.332491 -0.238914  0.527192  0.398873   \n",
      "2  -0.501537  0.389650  0.343678  ...  0.126471 -0.516269  0.851194  0.096332   \n",
      "3   0.012270  1.147934  0.579455  ...  0.034755 -0.948260 -0.083332 -0.063656   \n",
      "4  -1.402594 -0.211867  0.515534  ... -0.699512  0.467173  0.527516  0.126043   \n",
      "5   0.713021  1.102427 -0.607174  ...  0.603891 -0.759447  0.253610  0.412645   \n",
      "6   0.518045  0.051630 -0.295776  ... -0.909066 -0.279693  0.047460  0.542144   \n",
      "7   0.807235  0.313656  0.954144  ... -0.468571  0.060249  0.292981 -0.527562   \n",
      "8  -0.628873  0.465749  0.290690  ... -0.028737 -0.504688  0.480474  0.114519   \n",
      "9   0.177804  0.364214  0.373078  ... -0.563304 -0.474470  1.071362 -0.503738   \n",
      "10 -1.291250  0.281192  0.446570  ... -0.273728 -0.704308  0.050343  0.357290   \n",
      "11 -0.537169 -0.621616  0.430523  ...  0.957920 -0.480918  0.984497  0.245929   \n",
      "12 -0.127839  0.553243  1.073517  ... -1.225238 -0.000741  0.675144  1.161002   \n",
      "13  0.379611 -0.117743  0.872178  ...  0.515513  0.103992  0.454240  0.107733   \n",
      "14 -0.369221 -0.124605  0.563952  ... -0.072008 -0.306354  1.175374  0.958981   \n",
      "15 -0.692977 -0.528264  0.172130  ... -0.274170 -0.559978  1.080937  0.326006   \n",
      "\n",
      "      100063    100064    100065    100066    100067    100068  \n",
      "0  -0.682058 -0.401196  0.965412 -0.456405  0.027232 -0.635172  \n",
      "1  -0.217187 -0.907683  0.311409 -0.130067  0.143546  0.347755  \n",
      "2   0.299119 -0.208899 -0.740072 -0.516282  0.290687  0.561794  \n",
      "3   0.741069 -0.073434 -0.676253 -0.049096 -0.574581 -0.818222  \n",
      "4   0.810911 -0.035747  0.091597 -0.340363 -0.809907 -0.408865  \n",
      "5   0.498036 -0.853744  0.443934 -0.874876 -1.072931 -1.002424  \n",
      "6  -0.053423  0.322980 -0.586207 -0.453708 -0.619176 -0.932294  \n",
      "7   0.296405  0.029541 -1.007655 -0.122182 -0.406276 -2.064882  \n",
      "8   0.516496 -1.029752 -0.204067 -0.659740 -1.224719 -1.030573  \n",
      "9   0.433932 -0.462199  0.643427 -0.067032 -0.598720 -1.303403  \n",
      "10  0.580364 -0.020191  0.285560 -0.503810  0.178705 -0.607700  \n",
      "11  0.062951 -1.000433  0.090657 -0.827080 -0.487477  0.377111  \n",
      "12 -0.816465  0.781898  0.227989  0.272664 -0.416483 -0.864891  \n",
      "13 -0.022452  0.408057 -0.465433 -0.952858  0.018825 -0.110261  \n",
      "14 -0.586264 -0.148161 -0.215177 -1.215729  0.141848  0.051383  \n",
      "15  0.044286 -0.138977  0.417314 -1.024239  0.204148 -0.047354  \n",
      "\n",
      "[16 rows x 100069 columns]\n"
     ]
    }
   ],
   "source": [
    "logits = nn.Linear(d_model, max_token_value)(output)\n",
    "print(pd.DataFrame(logits[0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17680dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.softmax usually used during inference, during training we use torch.nn.CrossEntropyLoss\n",
    "# but for illustration purpose, we'll use torch.softmax here\n",
    "probabilities = torch.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71277ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
